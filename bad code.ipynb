{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for language: gujarati\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "is_valid_url() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 333\u001b[0m\n\u001b[0;32m    237\u001b[0m language_urls \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    238\u001b[0m      \u001b[38;5;66;03m# \"hindi\": {\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m#     \"base\": \"https://hindi.krishijagran.com/\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# },\u001b[39;00m\n\u001b[0;32m    330\u001b[0m }\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Start the scraping process and get the results\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m scraped_data \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28mprint\u001b[39m(scraped_data)\n",
      "Cell \u001b[1;32mIn[5], line 134\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(language_urls)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Scrape the main page\u001b[39;00m\n\u001b[0;32m    133\u001b[0m main_posts \u001b[38;5;241m=\u001b[39m scrape_section_page(base_url, main_class)\n\u001b[1;32m--> 134\u001b[0m khabar_posts \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_section_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Scrape the weather page if available\u001b[39;00m\n\u001b[0;32m    137\u001b[0m weather_posts \u001b[38;5;241m=\u001b[39m scrape_section_page(base_url, weather_class)\n",
      "Cell \u001b[1;32mIn[5], line 81\u001b[0m, in \u001b[0;36mscrape_section_page\u001b[1;34m(section_url, class_names)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m news_links:\n\u001b[0;32m     80\u001b[0m     link_url \u001b[38;5;241m=\u001b[39m urljoin(section_url, link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_valid_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_url\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     82\u001b[0m         news_links_text \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     83\u001b[0m         section_posts\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: link_url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: news_links_text})\n",
      "\u001b[1;31mTypeError\u001b[0m: is_valid_url() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# def scrape_section_page(section_url, class_names):\n",
    "#     response = requests.get(section_url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "#     section_posts = []\n",
    "#     for class_name in class_names:\n",
    "#         if class_name == \"none\":\n",
    "#             continue\n",
    "#         news_list_section = soup.find_all('div', class_=class_name)\n",
    "#         for news_div in news_list_section:\n",
    "#             news_links = news_div.find_all('a', href=True, title=True)\n",
    "#             for link in news_links:\n",
    "#                 post_title = link['title'].strip()\n",
    "#                 post_url = urljoin(section_url, link['href'])\n",
    "#                 if is_valid_url(post_url):\n",
    "#                     section_posts.append({'title': post_title, 'url': post_url})\n",
    "#                 else:\n",
    "#                     print(f\"Invalid URL found: {post_url}\")\n",
    "\n",
    "#     return section_posts\n",
    "\n",
    "# def scrape_section_page(section_url, class_names):\n",
    "#     response = requests.get(section_url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     section_posts = []\n",
    "#     for class_name in class_names:\n",
    "#         if class_name == \"none\":\n",
    "#             continue\n",
    "#         news_list_section = soup.find_all('div', class_=class_name)\n",
    "#         for news_div in news_list_section:\n",
    "#             news_links = news_div.find_all('a', href=True, title=True)\n",
    "#             for link in news_links:\n",
    "#                 post_title = link['title'].strip()\n",
    "#                 post_url = link['href']\n",
    "#                 if is_valid_url(post_url, section_url):\n",
    "#                     section_posts.append({'title': post_title, 'url': urljoin(section_url, post_url)})\n",
    "#                 else:\n",
    "#                     print(f\"Invalid URL found: {urljoin(section_url, post_url)}\")\n",
    "\n",
    "#     return section_posts\n",
    "\n",
    "# def scrape_section_page(section_url, class_names):\n",
    "#     response = requests.get(section_url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "#     section_posts = []\n",
    "#     for class_name in class_names:\n",
    "#         if class_name == \"none\":\n",
    "#             continue\n",
    "#         news_list_section = soup.find_all('div', class_=class_name)\n",
    "#         for news_div in news_list_section:\n",
    "#             news_links = news_div.find_all('a', href=True, title=True)\n",
    "#             for link in news_links:\n",
    "#                 post_title = link['title'].strip()\n",
    "#                 post_url = link['href']\n",
    "#                 if is_valid_url(post_url, section_url):\n",
    "#                     section_posts.append({'title': post_title, 'url': urljoin(section_url, post_url)})\n",
    "#                 else:\n",
    "#                     print(f\"Invalid URL found: {urljoin(section_url, post_url)}\")\n",
    "\n",
    "#     return section_posts\n",
    "\n",
    "def scrape_section_page(section_url, class_names):\n",
    "    response = requests.get(section_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    section_posts = []\n",
    "    for class_name in class_names:\n",
    "        if class_name == \"none\":\n",
    "            continue\n",
    "        news_list_section = soup.find_all('div', class_=class_name)\n",
    "        for news_div in news_list_section:\n",
    "            news_links = news_div.find_all('a', href=True)\n",
    "            for link in news_links:\n",
    "                link_url = urljoin(section_url, link.get('href', ''))\n",
    "                if is_valid_url(link_url, section_url):\n",
    "                    news_links_text = link.get_text().strip()\n",
    "                    section_posts.append({'url': link_url, 'title': news_links_text})\n",
    "\n",
    "    return section_posts\n",
    "\n",
    "def is_valid_url(url, base_url):\n",
    "    parsed = urlparse(urljoin(base_url, url))\n",
    "    return all([parsed.scheme, parsed.path, parsed.netloc])\n",
    "\n",
    "def scrape_post_content(post_url):\n",
    "    response = requests.get(post_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for auth_div in soup.find_all('div', class_='auth-name-dt'):\n",
    "        auth_div.decompose()\n",
    "    for author_div in soup.find_all('div', class_='h-author'):\n",
    "        author_div.decompose()\n",
    "    for author_div in soup.find_all('div', class_='col-md-4'):\n",
    "        author_div.decompose()\n",
    "    for author_div in soup.find_all('div', class_='d-mags'):\n",
    "        author_div.decompose()\n",
    "    for author_div in soup.find_all('div', class_='d-social'):\n",
    "        author_div.decompose()\n",
    "    for author_div in soup.find_all('div', class_='d-nav-item-info'):\n",
    "        author_div.decompose()\n",
    "\n",
    "    # Extract content\n",
    "    paragraphs = soup.find_all('p')\n",
    "    content = \"\\n\\n\".join([p.text.strip() for p in paragraphs])\n",
    "    \n",
    "    return content\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return all([parsed.scheme, parsed.netloc])\n",
    "\n",
    "def main(language_urls):\n",
    "    all_results = {}\n",
    "\n",
    "    for lang, urls in language_urls.items():\n",
    "        print(f\"Scraping data for language: {lang}\")\n",
    "        \n",
    "        base_url = urls[\"base\"]\n",
    "        weather_class = urls[\"weather_class\"]\n",
    "        khati_class = urls[\"khati_badi_class\"]\n",
    "        main_class = urls[\"main_class\"]\n",
    "        news_classes = urls[\"news_classes\"]\n",
    "        government_classes = urls[\"government_class\"]\n",
    "        \n",
    "        # Scrape the main page\n",
    "        main_posts = scrape_section_page(base_url, main_class)\n",
    "        khabar_posts = scrape_section_page(base_url, news_classes)\n",
    "\n",
    "        # Scrape the weather page if available\n",
    "        weather_posts = scrape_section_page(base_url, weather_class)\n",
    "\n",
    "        # Scrape the Khati Badi page\n",
    "        khati_posts = scrape_section_page(base_url, khati_class)\n",
    "\n",
    "        government_post = scrape_section_page(base_url, government_classes)\n",
    "\n",
    "        # Set to track processed URLs\n",
    "        processed_urls = set()\n",
    "\n",
    "        # Dictionary to store the results for this language\n",
    "        results = {\n",
    "            \"Main Page Posts\": [],\n",
    "            \"Khabar Section Posts\": [],\n",
    "            \"Weather Posts\": [],\n",
    "            \"Khati Badi Posts\": [],\n",
    "            \"government posts\":[]\n",
    "        }\n",
    "\n",
    "        # Process main page posts\n",
    "        for post in main_posts:\n",
    "            if post['url'] not in processed_urls:\n",
    "                content = scrape_post_content(post['url'])\n",
    "                results[\"Main Page Posts\"].append({\n",
    "                    \"title\": post['title'],\n",
    "                    \"link\": post['url'],\n",
    "                    \"content\": content\n",
    "                })\n",
    "                processed_urls.add(post['url'])     \n",
    "\n",
    "        # Process \"Khabar\" section posts\n",
    "        for post in khabar_posts:\n",
    "            if post['url'] not in processed_urls:\n",
    "                content = scrape_post_content(post['url'])\n",
    "                results[\"Khabar Section Posts\"].append({\n",
    "                    \"title\": post['title'],\n",
    "                    \"link\": post['url'],\n",
    "                    \"content\": content\n",
    "                })\n",
    "                processed_urls.add(post['url'])\n",
    "\n",
    "        # Process weather posts\n",
    "        for post in weather_posts:\n",
    "            if post['url'] not in processed_urls:\n",
    "                content = scrape_post_content(post['url'])\n",
    "                results[\"Weather Posts\"].append({\n",
    "                    \"title\": post['title'],\n",
    "                    \"link\": post['url'],\n",
    "                    \"content\": content\n",
    "                })\n",
    "                processed_urls.add(post['url'])\n",
    "\n",
    "        # Process Khati Badi posts\n",
    "        for post in khati_posts:\n",
    "            if post['url'] not in processed_urls:\n",
    "                content = scrape_post_content(post['url'])\n",
    "                results[\"Khati Badi Posts\"].append({\n",
    "                    \"title\": post['title'],\n",
    "                    \"link\": post['url'],\n",
    "                    \"content\": content\n",
    "                })\n",
    "                processed_urls.add(post['url'])\n",
    "       \n",
    "        for post in government_post:\n",
    "            if \"asomiya\" in language_urls or \"odia\" in language_urls:\n",
    "                if post['url'] not in processed_urls:\n",
    "                    if \"https://telugu.krishijagran.com/government-schemes\" in post['url']:\n",
    "                        content = scrape_post_content(post['url'])\n",
    "                        results[\"government posts\"].append({\n",
    "                            \"title\": post['title'],\n",
    "                            \"link\": post['url'],\n",
    "                            \"content\": content\n",
    "                        })\n",
    "                        processed_urls.add(post['url'])\n",
    "\n",
    "            if \"asomiya\" in language_urls or \"odia\" in language_urls:\n",
    "                if post['url'] not in processed_urls:\n",
    "                    if \"https://asomiya.krishijagran.com/government-schemes\" in post['url'] or \"https://odia.krishijagran.com/government-schemes/\" in post['url']:\n",
    "                        content = scrape_post_content(post['url'])\n",
    "                        results[\"government posts\"].append({\n",
    "                            \"title\": post['title'],\n",
    "                            \"link\": post['url'],\n",
    "                            \"content\": content\n",
    "                        })\n",
    "                        processed_urls.add(post['url'])\n",
    "            else:\n",
    "                if post['url'] not in processed_urls:\n",
    "                    content = scrape_post_content(post['url'])\n",
    "                    results[\"government posts\"].append({\n",
    "                        \"title\": post['title'],\n",
    "                        \"link\": post['url'],\n",
    "                        \"content\": content\n",
    "                    })\n",
    "                    processed_urls.add(post['url'])\n",
    "\n",
    "        all_results[lang] = results\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Define the base URLs and class names for each language\n",
    "language_urls = {\n",
    "     # \"hindi\": {\n",
    "    #     \"base\": \"https://hindi.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"weather-home mt-5 mb-3\"],\n",
    "    #     \"khati_badi_class\": [\"home-2-3-lst\"],\n",
    "    # },\n",
    "    # \"english\": {\n",
    "    #     \"base\": \"https://krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"home-2-3-lst\"],\n",
    "    # },\n",
    "    # \"punjabi\": {\n",
    "    #     \"base\": \"https://punjabi.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"none\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"home-top-l\"],\n",
    "    #     \"khati_badi_class\": [\"home-2-3-lst\"],\n",
    "    # },\n",
    "    # \"marathi\": {\n",
    "    #     \"base\": \"https://marathi.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"weather-home mt-5 mb-3\"],\n",
    "    #     \"khati_badi_class\": [\"none\"],\n",
    "    # },\n",
    "    # \"tamil\": {\n",
    "    #     \"base\": \"https://tamil.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"home-2-3-lst\"],\n",
    "    # },\n",
    "    # \"malayam\": {\n",
    "    #     \"base\": \"https://malayalam.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"none\"],\n",
    "    # },\n",
    "    #   \"bengali\": {\n",
    "    #     \"base\": \"https://bengali.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"weather-home mt-5 mb-3\"],\n",
    "    #     \"khati_badi_class\": [\"none\"],\n",
    "    #     \"government_class\":[\"h-cat-lst shadow-sm\"]\n",
    "    # },\n",
    "    #   \"kannada\": {\n",
    "    #     \"base\": \"https://kannada.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"none\"],\n",
    "    #     \"government_class\":[\"h-cat-lst shadow-sm\"]\n",
    "    # },\n",
    "    #   \"odia\": {\n",
    "    #     \"base\": \"https://odia.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"three-boxes\"],\n",
    "    #     \"government_class\":[\"h-cat-lst shadow-sm\"]\n",
    "    # },\n",
    "    #   \"asomiya\": {\n",
    "    #     \"base\": \"https://asomiya.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"home-top-l\"],\n",
    "    #     \"news_classes\": [\"home-top-news-lst\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"none\"],\n",
    "    #     \"government_class\":[\"h-cat-lst shadow-sm\"],\n",
    "    #     # \"government_class\":[\"list-unstyled\"]\n",
    "    # },\n",
    "    \"gujarati\": {\n",
    "         \"base\": \"https://gujarati.krishijagran.com/\",\n",
    "         \"main_class\": [\"none\"],\n",
    "         \"news_classes\": [\"col-xs-12 col-sm-6 col-md-6 col-lg-6 cat-flex\"],\n",
    "         \"weather_class\": [\"none\"],\n",
    "         \"khati_badi_class\": [\"none\"],\n",
    "         \"government_class\":[\"col-xs-12 col-sm-6 col-md-6 col-lg-6 cat-flex\"]\n",
    "    },\n",
    "    #   \"telugu\": {\n",
    "    #     \"base\": \"https://telugu.krishijagran.com/\",\n",
    "    #     \"main_class\": [\"row e-h\"],\n",
    "    #     \"news_classes\": [\"h-cat-lst shadow-sm\"],\n",
    "    #     \"weather_class\": [\"none\"],\n",
    "    #     \"khati_badi_class\": [\"none\"],\n",
    "    #     \"government_class\":[\"col-xs-12 col-sm-6 col-md-6 col-lg-6 cat-flex\"],\n",
    "        # \"government_class\":[\"list-unstyled\"]\n",
    "    # },\n",
    "}\n",
    "\n",
    "# Start the scraping process and get the results\n",
    "scraped_data = main(language_urls)\n",
    "print(scraped_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
